{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# InfoGAN Training Notebook\n",
    "The ```train.py``` script changed into a jupyter notebook to make interactively tweaking the training and adjusting the data a bit easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InfoGANSettings(discriminator_lr=0.0002, style_size=62, force_grayscale=False, batch_size=64, generator='fc:1024,fc:7x7x128,reshape:7:7:128,deconv:4:2:64,deconv:4:2:1:sigmoid', categorical_cardinality=[10], dataset=None, infogan=True, categorical_lambda=1.0, continuous_lambda=1.0, fix_std=True, epochs=100, seed=1234, num_continuous=2, generator_lr=0.001, plot_every=200, discriminator='conv:4:2:64:lrelu,conv:4:2:128:lrelu,fc:1024:lrelu', scale_dataset=[28, 28], max_images=None, use_batch_norm=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import namedtuple\n",
    "sys.path.append('..') # add the parent directory to the path\n",
    "def create_gan_settings(**kwargs):\n",
    "    arg_list = kwargs.items()\n",
    "    arg_type = namedtuple('InfoGANSettings', [k for (k, _) in arg_list])\n",
    "    return arg_type(*[v for (_, v) in arg_list])\n",
    "\n",
    "args = create_gan_settings(epochs = 100, dataset = None,\n",
    "                           max_images = None, scale_dataset = [28, 28],\n",
    "                           batch_size = 64, generator_lr = 1e-3,\n",
    "                           discriminator_lr = 2e-4, categorical_lambda = 1.0,\n",
    "                           continuous_lambda = 1.0, categorical_cardinality = [10],\n",
    "                           generator = \"fc:1024,fc:7x7x128,reshape:7:7:128,deconv:4:2:64,deconv:4:2:1:sigmoid\",\n",
    "                           discriminator = \"conv:4:2:64:lrelu,conv:4:2:128:lrelu,fc:1024:lrelu\",\n",
    "                           num_continuous = 2, seed = 1234, style_size = 62, plot_every = 200,\n",
    "                           infogan = True, use_batch_norm = True, fix_std = True, force_grayscale = False)\n",
    "\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import infogan as ifg\n",
    "from infogan.tf_utils import scope_variables\n",
    "from infogan.categorical_grid_plots import CategoricalPlotter\n",
    "from infogan.tf_utils import (\n",
    "    scope_variables,\n",
    "    NOOP,\n",
    "    load_mnist_dataset,\n",
    "    run_network,\n",
    "    leaky_rectify,\n",
    ")\n",
    "from infogan.misc_utils import (\n",
    "    next_unused_name,\n",
    "    add_boolean_cli_arg,\n",
    "    create_progress_bar,\n",
    "    load_image_dataset,\n",
    ")\n",
    "from infogan.noise_utils import (\n",
    "    create_infogan_noise_sample,\n",
    "    create_gan_noise_sample,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Setup\n",
    "## Loading training data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "batch_size = args.batch_size\n",
    "n_epochs = args.epochs\n",
    "use_batch_norm = args.use_batch_norm\n",
    "fix_std = args.fix_std\n",
    "plot_every = args.plot_every\n",
    "use_infogan = args.infogan\n",
    "style_size = args.style_size\n",
    "categorical_cardinality = args.categorical_cardinality\n",
    "num_continuous = args.num_continuous\n",
    "generator_desc = args.generator\n",
    "discriminator_desc = args.discriminator\n",
    "if args.dataset is None:\n",
    "    assert args.scale_dataset == [28, 28]\n",
    "    X = ifg.load_mnist_dataset()\n",
    "    if args.max_images is not None:\n",
    "        X = X[:args.max_images]\n",
    "    dataset_name = \"mnist\"\n",
    "else:\n",
    "    scaled_image_width, scaled_image_height = args.scale_dataset\n",
    "\n",
    "    # load pngs and jpegs here\n",
    "    X = load_image_dataset(\n",
    "        args.dataset,\n",
    "        desired_width=scaled_image_width, # TODO(jonathan): pick up from generator or add a command line arg (either or)...\n",
    "        desired_height=scaled_image_height,\n",
    "        value_range=(0.0, 1.0),\n",
    "        max_images=args.max_images,\n",
    "        force_grayscale=args.force_grayscale\n",
    "    )\n",
    "    dataset_name = basename(args.dataset.rstrip(\"/\"))\n",
    "\n",
    "if use_infogan:\n",
    "    z_size = style_size + sum(categorical_cardinality) + num_continuous\n",
    "    sample_noise = ifg.create_infogan_noise_sample(\n",
    "        categorical_cardinality,\n",
    "        num_continuous,\n",
    "        style_size\n",
    "    )\n",
    "else:\n",
    "    z_size = style_size\n",
    "    sample_noise = create_gan_noise_sample(style_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Building Model\n",
    "The model is built from the arguments and the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator architecture\n",
      "Fully connected with num_outputs=1024 followed by relu\n",
      "Fully connected with num_outputs=6272 followed by relu\n",
      "Reshape to [7, 7, 128]\n",
      "Deconvolution with nkernels=4, stride=2, num_outputs=64 followed by relu\n",
      "Deconvolution with nkernels=4, stride=2, num_outputs=1 followed by sigmoid\n",
      "\n",
      "Generator produced images of shape (28, 28, 1)\n",
      "\n",
      "discriminator architecture\n",
      "Convolution with nkernels=4, stride=2, num_outputs=64 followed by lrelu\n",
      "Convolution with nkernels=4, stride=2, num_outputs=128 followed by lrelu\n",
      "Fully connected with num_outputs=1024 followed by lrelu\n",
      "\n"
     ]
    }
   ],
   "source": [
    "discriminator_lr = tf.get_variable(\n",
    "    \"discriminator_lr\", (),\n",
    "    initializer=tf.constant_initializer(args.discriminator_lr)\n",
    ")\n",
    "generator_lr = tf.get_variable(\n",
    "    \"generator_lr\", (),\n",
    "    initializer=tf.constant_initializer(args.generator_lr)\n",
    ")\n",
    "\n",
    "n_images, image_height, image_width, n_channels = X.shape\n",
    "\n",
    "discriminator_lr_placeholder = tf.placeholder(tf.float32, (), name=\"discriminator_lr\")\n",
    "generator_lr_placeholder = tf.placeholder(tf.float32, (), name=\"generator_lr\")\n",
    "assign_discriminator_lr_op = discriminator_lr.assign(discriminator_lr_placeholder)\n",
    "assign_generator_lr_op = generator_lr.assign(generator_lr_placeholder)\n",
    "\n",
    "## begin model\n",
    "true_images = tf.placeholder(\n",
    "    tf.float32,\n",
    "    [None, image_height, image_width, n_channels],\n",
    "    name=\"true_images\"\n",
    ")\n",
    "zc_vectors = tf.placeholder(\n",
    "    tf.float32,\n",
    "    [None, z_size],\n",
    "    name=\"zc_vectors\"\n",
    ")\n",
    "is_training_discriminator = tf.placeholder(\n",
    "    tf.bool,\n",
    "    [],\n",
    "    name=\"is_training_discriminator\"\n",
    ")\n",
    "is_training_generator = tf.placeholder(\n",
    "    tf.bool,\n",
    "    [],\n",
    "    name=\"is_training_generator\"\n",
    ")\n",
    "\n",
    "fake_images = ifg.generator_forward(\n",
    "    zc_vectors,\n",
    "    generator_desc,\n",
    "    is_training=is_training_generator,\n",
    "    name=\"generator\",\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "print(\"Generator produced images of shape %s\" % (fake_images.get_shape()[1:]))\n",
    "print(\"\")\n",
    "\n",
    "discriminator_fake = ifg.discriminator_forward(\n",
    "    fake_images,\n",
    "    discriminator_desc,\n",
    "    is_training=is_training_discriminator,\n",
    "    name=\"discriminator\",\n",
    "    use_batch_norm=use_batch_norm,\n",
    "    debug=True\n",
    ")\n",
    "prob_fake = discriminator_fake[\"prob\"]\n",
    "discriminator_true = ifg.discriminator_forward(\n",
    "    true_images,\n",
    "    discriminator_desc,\n",
    "    is_training=is_training_discriminator,\n",
    "    reuse=True,\n",
    "    name=\"discriminator\",\n",
    "    use_batch_norm=use_batch_norm\n",
    ")\n",
    "prob_true = discriminator_true[\"prob\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Objective Functions\n",
    "For the generator and discriminator the appropriate objective and loss functions need to be established"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# discriminator should maximize:\n",
    "ll_believing_fake_images_are_fake = tf.log(1.0 - prob_fake + ifg.TINY)\n",
    "ll_true_images = tf.log(prob_true + ifg.TINY)\n",
    "discriminator_obj = (\n",
    "    tf.reduce_mean(ll_believing_fake_images_are_fake) +\n",
    "    tf.reduce_mean(ll_true_images)\n",
    ")\n",
    "\n",
    "# generator should maximize:\n",
    "ll_believing_fake_images_are_real = tf.reduce_mean(tf.log(prob_fake + ifg.TINY))\n",
    "generator_obj = ll_believing_fake_images_are_real\n",
    "\n",
    "discriminator_solver = tf.train.AdamOptimizer(\n",
    "    learning_rate=discriminator_lr,\n",
    "    beta1=0.5\n",
    ")\n",
    "generator_solver = tf.train.AdamOptimizer(\n",
    "    learning_rate=generator_lr,\n",
    "    beta1=0.5\n",
    ")\n",
    "\n",
    "discriminator_variables = scope_variables(\"discriminator\")\n",
    "generator_variables = scope_variables(\"generator\")\n",
    "\n",
    "train_discriminator = discriminator_solver.minimize(-discriminator_obj, var_list=discriminator_variables)\n",
    "train_generator = generator_solver.minimize(-generator_obj, var_list=generator_variables)\n",
    "discriminator_obj_summary = tf.summary.scalar(\"discriminator_objective\", discriminator_obj)\n",
    "generator_obj_summary = tf.summary.scalar(\"generator_objective\", generator_obj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Tensorboard Outputs\n",
    "The outputs for tensorboard are configured so the results can be visualized there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "if use_infogan:\n",
    "    categorical_c_vectors = []\n",
    "    offset = 0\n",
    "    for cardinality in categorical_cardinality:\n",
    "        categorical_c_vectors.append(\n",
    "            zc_vectors[:, offset:offset+cardinality]\n",
    "        )\n",
    "        offset += cardinality\n",
    "\n",
    "    continuous_c_vector = zc_vectors[:, offset:offset + num_continuous]\n",
    "\n",
    "    q_output = ifg.reconstruct_mutual_info(\n",
    "        categorical_c_vectors,\n",
    "        continuous_c_vector,\n",
    "        categorical_lambda=args.categorical_lambda,\n",
    "        continuous_lambda=args.continuous_lambda,\n",
    "        fix_std=fix_std,\n",
    "        hidden=discriminator_fake[\"hidden\"],\n",
    "        is_training=is_training_discriminator,\n",
    "        name=\"mutual_info\"\n",
    "    )\n",
    "\n",
    "    mutual_info_objective = q_output[\"mutual_info\"]\n",
    "    mutual_info_variables = scope_variables(\"mutual_info\")\n",
    "    neg_mutual_info_objective = -mutual_info_objective\n",
    "    train_mutual_info = generator_solver.minimize(\n",
    "        neg_mutual_info_objective,\n",
    "        var_list=generator_variables + discriminator_variables + mutual_info_variables\n",
    "    )\n",
    "    ll_categorical = q_output[\"ll_categorical\"]\n",
    "    ll_continuous = q_output[\"ll_continuous\"]\n",
    "    std_contig = q_output[\"std_contig\"]\n",
    "\n",
    "    mutual_info_obj_summary = tf.summary.scalar(\"mutual_info_objective\", mutual_info_objective)\n",
    "    ll_categorical_obj_summary = tf.summary.scalar(\"ll_categorical_objective\", ll_categorical)\n",
    "    ll_continuous_obj_summary = tf.summary.scalar(\"ll_continuous_objective\", ll_continuous)\n",
    "    std_contig_summary = tf.summary.scalar(\"std_contig\", std_contig)\n",
    "    generator_obj_summary = tf.summary.merge([\n",
    "        generator_obj_summary,\n",
    "        mutual_info_obj_summary,\n",
    "        ll_categorical_obj_summary,\n",
    "        ll_continuous_obj_summary,\n",
    "        std_contig_summary\n",
    "    ])\n",
    "else:\n",
    "    neg_mutual_info_objective = NOOP\n",
    "    mutual_info_objective = NOOP\n",
    "    train_mutual_info = NOOP\n",
    "    ll_categorical = NOOP\n",
    "    ll_continuous = NOOP\n",
    "    std_contig = NOOP\n",
    "    entropy = NOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving tensorboard logs to 'D:\\\\CS7476\\\\proj_exp\\\\tensorflow-infogan\\\\mnist_log\\\\infogan-1'\n"
     ]
    }
   ],
   "source": [
    "log_dir = next_unused_name(\n",
    "    os.path.join(\n",
    "        ifg.PROJECT_DIR,\n",
    "        \"%s_log\" % (dataset_name,),\n",
    "        \"infogan\" if use_infogan else \"gan\"\n",
    "    )\n",
    ")\n",
    "journalist = tf.summary.FileWriter(\n",
    "    log_dir,\n",
    "    flush_secs=10\n",
    ")\n",
    "print(\"Saving tensorboard logs to %r\" % (log_dir,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "img_summaries = {}\n",
    "if use_infogan:\n",
    "    plotter = CategoricalPlotter(\n",
    "        categorical_cardinality=categorical_cardinality,\n",
    "        num_continuous=num_continuous,\n",
    "        style_size=style_size,\n",
    "        journalist=journalist,\n",
    "        generate=lambda sess, x: sess.run(\n",
    "            fake_images,\n",
    "            {zc_vectors:x, is_training_discriminator:False, is_training_generator:False}\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    image_placeholder = None\n",
    "    plotter = None\n",
    "    img_summaries[\"fake_images\"] = tf.summary.image(\"fake images\", fake_images, max_images=10)\n",
    "image_summary_op = tf.summary.merge(list(img_summaries.values())) if len(img_summaries) else NOOP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0 >> 34   3% |#                                            |ETA:  0:27:44\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1d462c28b143>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     39\u001b[0m                     \u001b[0mzc_vectors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m                     \u001b[0mis_training_discriminator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m                     \u001b[0mis_training_generator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m                 }\n\u001b[0;32m     43\u001b[0m             )\n",
      "\u001b[1;32mC:\\Users\\axeisghost\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\axeisghost\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\axeisghost\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Users\\axeisghost\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\axeisghost\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "idxes = np.arange(n_images, dtype=np.int32)\n",
    "iters = 0\n",
    "with tf.Session() as sess:\n",
    "    # pleasure\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    # content\n",
    "    for epoch in range(n_epochs):\n",
    "        disc_epoch_obj = []\n",
    "        gen_epoch_obj = []\n",
    "        infogan_epoch_obj = []\n",
    "\n",
    "        np.random.shuffle(idxes)\n",
    "        pbar = create_progress_bar(\"epoch %d >> \" % (epoch,))\n",
    "\n",
    "        for idx in pbar(range(0, n_images, batch_size)):\n",
    "            batch = X[idxes[idx:idx + batch_size]]\n",
    "            # train discriminator\n",
    "            noise = sample_noise(batch_size)\n",
    "            _, summary_result1, disc_obj, infogan_obj = sess.run(\n",
    "                [train_discriminator, discriminator_obj_summary, discriminator_obj, neg_mutual_info_objective],\n",
    "                feed_dict={\n",
    "                    true_images:batch,\n",
    "                    zc_vectors:noise,\n",
    "                    is_training_discriminator:True,\n",
    "                    is_training_generator:True\n",
    "                }\n",
    "            )\n",
    "\n",
    "            disc_epoch_obj.append(disc_obj)\n",
    "\n",
    "            if use_infogan:\n",
    "                infogan_epoch_obj.append(infogan_obj)\n",
    "\n",
    "            # train generator\n",
    "            noise = sample_noise(batch_size)\n",
    "            _, _, summary_result2, gen_obj, infogan_obj = sess.run(\n",
    "                [train_generator, train_mutual_info, generator_obj_summary, generator_obj, neg_mutual_info_objective],\n",
    "                feed_dict={\n",
    "                    zc_vectors:noise,\n",
    "                    is_training_discriminator:True,\n",
    "                    is_training_generator:True\n",
    "                }\n",
    "            )\n",
    "\n",
    "            journalist.add_summary(summary_result1, iters)\n",
    "            journalist.add_summary(summary_result2, iters)\n",
    "            journalist.flush()\n",
    "            gen_epoch_obj.append(gen_obj)\n",
    "\n",
    "            if use_infogan:\n",
    "                infogan_epoch_obj.append(infogan_obj)\n",
    "\n",
    "            iters += 1\n",
    "\n",
    "            if iters % plot_every == 0:\n",
    "                if use_infogan:\n",
    "                    plotter.generate_images(sess, 10, iteration=iters)\n",
    "                else:\n",
    "                    noise = sample_noise(batch_size)\n",
    "                    current_summary = sess.run(\n",
    "                        image_summary_op,\n",
    "                        {\n",
    "                            zc_vectors:noise,\n",
    "                            is_training_discriminator:False,\n",
    "                            is_training_generator:False\n",
    "                        }\n",
    "                    )\n",
    "                    journalist.add_summary(current_summary, iters)\n",
    "                journalist.flush()\n",
    "\n",
    "        msg = \"epoch %d >> discriminator LL %.2f (lr=%.6f), generator LL %.2f (lr=%.6f)\" % (\n",
    "            epoch,\n",
    "            np.mean(disc_epoch_obj), sess.run(discriminator_lr),\n",
    "            np.mean(gen_epoch_obj), sess.run(generator_lr)\n",
    "        )\n",
    "        if use_infogan:\n",
    "            msg = msg + \", infogan loss %.2f\" % (np.mean(infogan_epoch_obj),)\n",
    "        print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Show the tensorboard results\n",
    "!tensorboard --logdir mnist_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Reset everything to initial state\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
